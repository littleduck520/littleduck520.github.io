+++
date = '2025-12-03T01:04:38+08:00'
draft = false
title = 'DeepLearnProuts'

+++

# # 第一周深度学习基础

## 【第一部分：代码练习】

### 3.1 pytorch基础练习

[尝试链接]([Google Colab](https://colab.research.google.com/drive/1SuWdRTvEA15_l76ppUxDPie4WfjQ-sPk))

- **定义数据**

<img src="1.png" style="zoom:40;" />

什么是 Tensor？

- **定义：** `torch.Tensor` (张量) 是 PyTorch 中用于存储和操作数据的基本数据结构。

- **形式：** 它可以表示一个**数**（标量）、**一维数组**（向量）、**二维数组**（矩阵），以及**任意维度**的数组。

  

| **描述**                    | **语法**                       | **示例**                                                     |
| --------------------------- | ------------------------------ | ------------------------------------------------------------ |
| **直接定义** (从现有数据)   | `torch.tensor(data)`           | `x = torch.tensor(666)` (标量)   `x = torch.tensor([1, 2, 3])` (向量) |
| **全1张量**                 | `torch.ones(size)`             | `x = torch.ones(2, 3)`                                       |
| **全0张量**                 | `torch.zeros(size, dtype=...)` | `x = torch.zeros(5, 3, dtype=torch.long)`                    |
| **空张量** (未初始化)       | `torch.empty(size)`            | `x = torch.empty(5, 3)`                                      |
| **随机张量** (均匀分布)     | `torch.rand(size)`             | `x = torch.rand(5, 3)`                                       |
| **随机张量** (标准正态分布) | `torch.randn(size)`            | -                                                            |



- **定义操作**

<img src="2.png" style="zoom:40%;" />

1. Tensor属性和索引

   | **描述**                     | **语法**                         | **示例**                 | **结果示例**               |
   | ---------------------------- | -------------------------------- | ------------------------ | -------------------------- |
   | **获取维度大小** (行数/列数) | `tensor.size(dim)`               | `m.size(0)`, `m.size(1)` | `2`, `4`                   |
   | **获取完整形状**             | `tensor.size()`                  | `m.size()`               | `torch.Size([2, 4])`       |
   | **获取元素总数**             | `tensor.numel()`                 | `m.numel()`              | `8`                        |
   | **索引单个元素**             | `tensor[i][j]` 或 `tensor[i, j]` | `m[0][2]`                | `tensor(3.)`               |
   | **切片 (取整列)**            | `tensor[:, j]`                   | `m[:, 1]`                | `tensor([5., 2.])`         |
   | **切片 (取整行)**            | `tensor[i, :]`                   | `m[0, :]`                | `tensor([2., 5., 3., 7.])` |

2. 常见创造操作

   | **描述**                  | **语法**                            | **示例**                   | **结果特点**                               |
   | ------------------------- | ----------------------------------- | -------------------------- | ------------------------------------------ |
   | **创建等差序列** (整数)   | `torch.arange(start, end)`          | `v = torch.arange(1, 5)`   | **不包含 `end`** (即 1 到 4)               |
   | **创建等距序列** (浮点数) | `torch.linspace(start, end, steps)` | `torch.linspace(3, 8, 20)` | **包含 `start` 和 `end`**，共 `steps` 个点 |

3. 基本数学和线性代数运算

   | **描述**       | **语法**                                            | **示例**                     | **备注**                 |
   | -------------- | --------------------------------------------------- | ---------------------------- | ------------------------ |
   | **元素级加法** | `tensor1 + tensor2`                                 | `m + torch.rand(2, 4)`       | 要求形状一致（或可广播） |
   | **矩阵乘法**   | `tensor1 @ tensor2` 或 `torch.mm(tensor1, tensor2)` | `m @ v`                      | 要求内维度匹配           |
   | **转置**       | `tensor.t()` 或 `tensor.transpose(dim0, dim1)`      | `m.t()`, `m.transpose(0, 1)` | `m.t()` 适用于 2D Tensor |

4. 运算分类

   | **运算类型**      | **常用函数示例**                                             |
   | ----------------- | ------------------------------------------------------------ |
   | **基本运算**      | `abs`, `sqrt`, `div`, `exp`, `pow`, `fmod`, `cos`, `sin`, `ceil`, `round`, `floor` |
   | **布尔运算/统计** | `gt`, `lt`, `eq`, `ne`, `max`, `min`, `topk`, `sort`         |
   | **线性代数**      | `mm` (矩阵乘法), `bmm` (批量矩阵乘法), `t` (转置), `dot`, `cross`, `inverse`, `svd` |

5. Tensor 与 NumPy 的相互转换

   <img src="3.png" style="zoom:40%;" />

   

### 3.2螺旋数据分类

[尝试链接]([week1_2spiraldate.ipynb - Colab](https://colab.research.google.com/drive/15Ci0NneFRNpvgaRGK7YuJxxJD5Ob1HuY#scrollTo=TH5n4_qBNHDM))

直观地演示“线性模型”和“非线性神经网络”的区别，以及激活函数的重要性



**线性模型的准确率较低**

<img src="4.png" style="zoom:50%;" />

**添加Relu激活函数**

<img src="5.png" style="zoom:50%;" />

## 【第⼆部分：问题总结】

*1、AlexNet有哪些特点？为什么可以比LeNet取得更好的性能？*

​	AlexNet有更深的网络结构： AlexNet 共有 8 层（5 个卷积层 + 3 个全连接层），比早期的 LeNet（通常只有 5 层）要深得多。他使用 ReLU 激活函数,作为非线性激活函数。

​	优势： 大大加快了网络的训练速度，同时缓解了梯度消失问题。

​	使用 GPU 并行计算： AlexNet 是第一个证明使用多块 GPU 进行并行计算可以有效训练大规模深度卷积神经网络的模型。

​	大规模数据增强： 使用了大量的数据增强技术（如随机裁剪、水平翻转等），以增加训练集的多样性，提高泛化能力。

*2、激活函数有哪些作⽤？*

​	激活函数是深度学习的基石，核心作用是：引入非线性（Non-linearity）。

如果没有激活函数，无论神经网络有多少层，它都只是在进行线性变换（Wx+b）。多层线性变换的堆叠最终仍然只是一个单一的线性函数。引入激活函数后，网络才具备拟合非线性函数的能力，使其可以学习和模拟现实世界中复杂、弯曲的映射关系

​	增强模型的表达能力：通过非线性变换，网络能够提取和组合输入数据中更高级、更抽象的特征。

​	约束输出范围

*3、梯度消失现象是什么？*

梯度消失现象是指在训练深层神经网络时，由于反向传播机制，靠近输入层的参数梯度会变得非常小甚至趋近于零，导致这些层几乎无法获得有效的更新信号。靠近输入层的权重（W）几乎停滞不前，不进行学习。

当有很多层时，许多小于 1 的数连续相乘，结果会变得指数级地小。

*4、神经网络是更宽好还是更深好？*

在现代深度学习中，在参数总量相当的情况下，更深（Deep）的网络通常优于更宽（Wide）的网络。

更深网络的优势:

​	特征层次结构（Hierarchical Feature Learning）：深层网络能够逐步学习从简单到复杂的抽象特征。例如：第一层学到边缘和角点；中间层学到纹理和局部图案；高层学到整体物体或概念。这种分层的特征表达能力对于复杂任务（如图像识别）至关重要。

​	参数效率（Parameter Efficiency）：研究表明，一个深层、窄的网络通常可以用比一个浅层、宽的网络更少的参数，来表达相同复杂度的函数。这意味着在计算资源有限的情况下，深层网络更高效。

​	更好的泛化能力：深度通常有助于模型更好地泛化到未见过的数据。

*5、为什么要使⽤Softmax?*

输出具有统计学意义，可以直接解释为模型对该输入属于每个类别的置信度（概率）。

配合损失函数：Softmax 的输出（概率）是计算交叉熵损失（Cross-Entropy Loss） 的必要输入。交叉熵损失是分类任务中最常用的损失函数之一，它度量了预测概率分布与真实标签分布之间的差异。

*6、SGD 和 Adam 哪个更有效？*

SGD基础优化器，通常搭配动量（Momentum）。所有参数使用统一的学习率。在某些大型视觉任务中，如果精心调优，有时能达到更好的最终泛化性能。需要手动调整学习率，收敛速度慢。

Adam自适应学习率（Adaptive Learning Rate）。为每个参数动态调整学习率，并结合了动量机制。收敛速度快，容易使用，是大多数任务的默认首选。有时被认为泛化性能略逊于经过精细调优的 SGD。

